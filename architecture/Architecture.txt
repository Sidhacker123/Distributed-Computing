ğŸ“¥ 1. Input Driver Layer (CLI or API)
    â”œâ”€â”€ Triggered via `main.py` (Dask) or `mpirun` (MPI)
    â”œâ”€â”€ Accepts matrix dimensions, number of GPUs, and execution flags
    â””â”€â”€ Directs flow to: single-GPU, Dask, or standalone MPI modes

ğŸš¦ 2. Control & Scheduling Layer
    â”œâ”€â”€ Python/Dask Path:
    â”‚   â””â”€â”€ `main.py` invokes `utils.start_cluster()` (Dask-CUDA)
    â”‚       â””â”€â”€ Sets up LocalCUDACluster with N workers
    â””â”€â”€ MPI Path:
        â””â”€â”€ Rank 0 node scatters A, B using CUDA-aware MPI (e.g. ring broadcast)

ğŸ›ï¸ 3. Execution Layer
    â”œâ”€â”€ Python-Dask Logic (in `utils.py`):
    â”‚   â”œâ”€â”€ Uses `cupy.RawKernel` to launch `matrixMultiplyShared` on each GPU
    â”‚   â”œâ”€â”€ Handles row-wise or block-wise partitioning of matrices
    â”‚   â””â”€â”€ Optionally uses `cupy.matmul` for fallback
    â””â”€â”€ MPI Standalone Kernel (in `cuda_mpi_matrix_mult.cu`):
        â”œâ”€â”€ Tiled matrix multiplication with shared memory
        â””â”€â”€ Uses full C++/CUDA kernel logic + MPI sync

ğŸ“ˆ 4. Profiling + Monitoring Layer
    â”œâ”€â”€ Single GPU: Uses `cudaEvent` for kernel timing inside `run_matrix_multiply_kernel`
    â”œâ”€â”€ Multi-GPU (Dask): Wraps compute in wall-clock profiler (Python)
    â””â”€â”€ MPI: Uses `cudaEvent` and MPI barrier for timing sync

ğŸ“¤ 5. Output Aggregation Layer
    â”œâ”€â”€ Dask: Gathers partial blocks or rows from futures into final NumPy array
    â””â”€â”€ MPI: Rank 0 collects result from all ranks and assembles matrix

ğŸ“Š 6. Benchmark + Reporting Layer
    â””â”€â”€ `benchmarks.py`: Runs tests from 1â€“N GPUs, records speedup, saves CSV

Distributed GPU Matrix Compute Architecture standalone MPI :

1. Input Driver (CLI Args or API Gateway)
   â””â”€â”€ Accepts Matrix Size and Node Count

2. MPI Controller Node
   â””â”€â”€ Broadcasts A and B matrices via ring-style CUDA-aware MPI

3. CUDA-Enabled Worker Nodes
   â””â”€â”€ Perform tiled matrix multiplication using shared memory
   â””â”€â”€ Each node handles a partition of the full result matrix

4. Profiling + Monitoring Layer
   â””â”€â”€ Measures latency using cudaEvent
   â””â”€â”€ Outputs node-wise compute performance

5. Output Aggregator
   â””â”€â”€ Gathers partial results and reconstructs final matrix (on rank 0)
